<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Dmitry Grishchenko</title>
    <link>https://grishchenko.org/post/</link>
      <atom:link href="https://grishchenko.org/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Grishchenko Dmitry  2020</copyright><lastBuildDate>Fri, 25 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://grishchenko.org/img/logo.png</url>
      <title>Posts</title>
      <link>https://grishchenko.org/post/</link>
    </image>
    
    <item>
      <title>A new article about asynchronous progressive hedging</title>
      <link>https://grishchenko.org/post/new_progressive/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/new_progressive/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Proximal Gradient Methods with Adaptive Subspace Samplings&lt;/p&gt;

&lt;p&gt;Together with &lt;a href = &#34;https://gbareilles.fr/&#34;&gt;Gilles Bareilles&lt;/a&gt;, &lt;a href = &#34;https://yassine-laguel.github.io/&#34;&gt;Yassine Laguel&lt;/a&gt;, &lt;a href = &#34;http://iutzeler.org&#34;&gt;Franck Iutzeler&lt;/a&gt;, and &lt;a href = &#34;https://ljk.imag.fr/membres/Jerome.Malick/&#34;&gt;Jérôme Malick&lt;/a&gt; we propose an asynchronous version of the Progressive Hedging Algorithm(a
popular strategy in multistage stochastic programming) that is able to compute an
update as soon as a scenario subproblem is solved. Based on the similar arguments
as in Asynchronous ADMM, we prove that the asynchronous version has the same
convergence properties as the standard one and we release an easy-to-use &lt;a href = &#34;https://github.com/yassine-laguel/RandomizedProgressiveHedging.jl&#34;&gt;Julia toolbox&lt;/a&gt; .&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rehearsal fo defence</title>
      <link>https://grishchenko.org/post/defence_rehearsal/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/defence_rehearsal/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On Tuesday, $29^{\text{th}}$ September I will make a rehearsal for my Ph.D. defence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Title:&lt;/em&gt;&lt;/strong&gt; Proximal Optimization with Automatic Dimension Reduction for Large Scale Learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Abstract:&lt;/em&gt;&lt;/strong&gt; In this thesis, we develop a framework to reduce the dimensionality of composite optimization problems with sparsity inducing regularizers. Based on the identification property of
proximal methods, we first develop a ``sketch-and-project” method that uses projections
based on the structure of the correct point. This method allows to work with random
low-dimensional subspaces instead of considering the full space in the cases when the
final solution is sparse. Second, we place ourselves in the context of the delay-tolerant
asynchronous proximal methods and use our dimension reduction technique to decrease
the total size of communications. However, this technique is proven to converge only for
well-conditioned problems both in theory in practice. Thus, we investigate wrapping it up
into a proximal reconditioning framework. This leads to a theoretically backed algorithm
that is guaranteed to cost less in terms of communications compared with a non-sparsified
version; we show in practice that it implies faster runtime convergence when the sparsity
of the problem is suficiently big.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Talk will take a place in Batiment IMAG in the room $\boldsymbol{106}$ (first floor) at $\boldsymbol{14^{30}}$ on Tuesday, $\boldsymbol{29^{\text{th}}}$ September.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;All are invited to attend.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P.S. If you have no badge to enter the building please contact me in advance.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new article about random proximal subspace method with identification-based sampling</title>
      <link>https://grishchenko.org/post/new_adaptive_sampling/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/new_adaptive_sampling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Proximal Gradient Methods with Adaptive Subspace Samplings&lt;/p&gt;

&lt;p&gt;Together with my supervisors &lt;a href = &#34;http://iutzeler.org&#34;&gt;Franck Iutzeler&lt;/a&gt; and &lt;a href = &#34;https://ljk.imag.fr/membres/Jerome.Malick/&#34;&gt;Jérôme Malick&lt;/a&gt; we propose a randomized proximal gradient method that uses enforced by regularizer low dimensional structure of the solution to specify the subspaces and improve the rate in terms of dimensions explored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new article about acceleration of adaptive methods.</title>
      <link>https://grishchenko.org/post/new_adaptive_catalyst/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/new_adaptive_catalyst/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Adaptive Catalyst for smooth convex optimization&lt;/p&gt;

&lt;p&gt;Together with &lt;a href = &#34;https://scholar.google.ru/citations?user=cxQzNooAAAAJ&amp;hl=en&amp;oi=ao&#34;&gt; Anastasiya Ivanova&lt;/a&gt;, &lt;a href =&#34;https://scholar.google.ru/citations?hl=en&amp;user=XlmSx18AAAAJ&#34;&gt; Egor Shulgin&lt;/a&gt;, and &lt;a href = &#34;https://scholar.google.ru/citations?hl=en&amp;user=AmeE8qkAAAAJ&#34;&gt;Alexander Gasnikov&lt;/a&gt; from MIPT research group, we propose the universal acceleration technique for adaptive methods for smooth convex but not strongly convex objective functions. It allows accelerating such well-known methods as Steepest Descent, Random Adaptive Coordinate Descent Method, and others where Lipschitz constant of the gradient is either unknown(expensive to compute) or changes a lot along the method trajectory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I am visiting Peter Richtárik at KAUST</title>
      <link>https://grishchenko.org/post/kaust_visit/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/kaust_visit/</guid>
      <description>&lt;p&gt;From 1 of November to 21 of December I am joining &lt;a href = &#34;https://richtarik.org&#34;&gt;Peter Richtárik&#39;s&lt;/a&gt; team at KAUST as a visiting PhD student.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk of Nikita Doikov</title>
      <link>https://grishchenko.org/post/nikita_visit/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://grishchenko.org/post/nikita_visit/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On Monday, $23^{\text{rd}}$ September &lt;a href=&#34;https://scholar.google.be/citations?user=YNBhhjUAAAAJ&amp;hl=en&amp;oi=ao&#34;&gt;Nikita Doikov&lt;/a&gt; will give a talk
about some recent results on accelerated proximal methods.&lt;/p&gt;

&lt;p&gt;Nikita is a PhD student at Université Catholique de Louvain, Belgium, where he is working on developing high-order optimization methods in the group of Prof. Yurii Nesterov.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Title:&lt;/em&gt;&lt;/strong&gt; Proximal Method with Contractions for Smooth Convex Optimization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Abstract:&lt;/em&gt;&lt;/strong&gt; In this work we study a construction of proximal accelerated methods for smooth convex optimization. At every step the method minimizes a sum of contracted objective and a regularizer in a form of Bregman divergence, by some auxiliary subroutine. We provide a simple practical stopping criterion for solving the subproblem and present complexity analysis for a general scheme. In the case when Tensor Method is used as internal method, we show accelerated rate of convergence in both convex and strongly convex composite cases with additional logarithmic factor as a cost for solving the subproblem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Talk will take a place in Batiment IMAG in the room $\boldsymbol{106}$ (first floor) at $\boldsymbol{14^{00}}$ on Monday, $\boldsymbol{23^{\text{rd}}}$ September.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;All are invited to attend.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P.S. If you have no badge to enter the building please contact me in advance.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
