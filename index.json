[{"authors":["admin"],"categories":null,"content":"I am a second year PhD student at Université Grenoble Alpes supervised by Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini.\nI am interested in optimization algorithms for large-scale applications, more precisely in first-order ones that are focused on sparsification of communications in distributed setting.\nIn free time I am interested in basketball, workout, swimming, hiking, climbing. In addition, I am also a member of Russian first team in different logic puzzles and sudoku competitions.\n","date":1544400000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1544400000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://grishchenko.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a second year PhD student at Université Grenoble Alpes supervised by Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini.\nI am interested in optimization algorithms for large-scale applications, more precisely in first-order ones that are focused on sparsification of communications in distributed setting.\nIn free time I am interested in basketball, workout, swimming, hiking, climbing. In addition, I am also a member of Russian first team in different logic puzzles and sudoku competitions.","tags":null,"title":"Dmitry Grishchenko","type":"authors"},{"authors":null,"categories":null,"content":"Hello everyone,\nOn Monday, $23^{\\text{rd}}$ September Nikita Doikov will give a talk about some recent results on accelerated proximal methods.\nNikita is a PhD student at Université Catholique de Louvain, Belgium, where he is working on developing high-order optimization methods in the group of Prof. Yurii Nesterov.\nTitle: Proximal Method with Contractions for Smooth Convex Optimization.\nAbstract: In this work we study a construction of proximal accelerated methods for smooth convex optimization. At every step the method minimizes a sum of contracted objective and a regularizer in a form of Bregman divergence, by some auxiliary subroutine. We provide a simple practical stopping criterion for solving the subproblem and present complexity analysis for a general scheme. In the case when Tensor Method is used as internal method, we show accelerated rate of convergence in both convex and strongly convex composite cases with additional logarithmic factor as a cost for solving the subproblem.\nTalk will take a place in Batiment IMAG in the room $\\boldsymbol{106}$ (first floor) at $\\boldsymbol{14^{00}}$ on Monday, $\\boldsymbol{23^{\\text{rd}}}$ September.\nAll are invited to attend.\nP.S. If you have no badge to enter the building please contact me in advance.\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"4c4d45ef794d4c51c04fa2f06132a9a0","permalink":"https://grishchenko.org/post/nikita_visit/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/post/nikita_visit/","section":"post","summary":"Hello everyone,\nOn Monday, $23^{\\text{rd}}$ September Nikita Doikov will give a talk about some recent results on accelerated proximal methods.\nNikita is a PhD student at Université Catholique de Louvain, Belgium, where he is working on developing high-order optimization methods in the group of Prof. Yurii Nesterov.\nTitle: Proximal Method with Contractions for Smooth Convex Optimization.\nAbstract: In this work we study a construction of proximal accelerated methods for smooth convex optimization.","tags":null,"title":"Talk of Nikita Doikov","type":"post"},{"authors":null,"categories":null,"content":"","date":1565100300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565100300,"objectID":"f98655a5eef35cd0b9921d35c9cc0483","permalink":"https://grishchenko.org/talk/iccopt2019/","publishdate":"2019-08-06T14:05:00Z","relpermalink":"/talk/iccopt2019/","section":"talk","summary":"In this talk, we present a first-order optimization algorithm for distributed learning problems. We propose an efficient sparsification of proximal-gradient updates to lift the communication bottleneck of exchanging huge size updates between master machine and workers machines. Using a sketch-and-project technique with projections onto near-optimal low-dimensional subspaces, we get a significant performance improvement in terms of convergence with respect to the total size of communication.","tags":null,"title":"Identification-Based First-Order Algorithms for Distributed Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1562241900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562241900,"objectID":"c90e73712aa32fa58954a5841bec8220","permalink":"https://grishchenko.org/talk/spars2019/","publishdate":"2019-07-04T12:05:00Z","relpermalink":"/talk/spars2019/","section":"talk","summary":"Many machine learning and signal processing applications involve high-dimensional nonsmooth optimization problems. The nonsmoothness is essential as it brings a low-dimensional structure to the optimal solutions, as (block, rank, or variation) sparsity. In this work, we exploit this nonsmoothness to reduce the communication cost of optimization algorithms solving these problems in a distributed setting. We introduce two key ideas$:$ i) a random subspace descent algorithm along sparsity directions; ii) an adaptative subspace selection based on sparsity identification of the proximal operator. We get significant performance improvements in terms of convergence with respect to data exchanged.","tags":null,"title":"Distributed First-Order Optimization with Tamed Communications","type":"talk"},{"authors":null,"categories":null,"content":"","date":1554820200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554820200,"objectID":"afb4b9707224250e1756923c4da74815","permalink":"https://grishchenko.org/talk/dao2019/","publishdate":"2019-04-09T14:30:00Z","relpermalink":"/talk/dao2019/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Identify and Sparsify$:$ Distributed Optimization with Asynchronous Moderate Communications","type":"talk"},{"authors":null,"categories":null,"content":"    ","date":1553704200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553704200,"objectID":"6d0eb8ae48ccebeab0be4d447d47b040","permalink":"https://grishchenko.org/talk/osl2019/","publishdate":"2019-03-27T16:30:00Z","relpermalink":"/talk/osl2019/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Identify and Sparsify$:$ Distributed Optimization with Asynchronous Moderate Communications","type":"talk"},{"authors":null,"categories":null,"content":"Grenoble INP \u0026ndash; masters of ENSIMAG (TA 20h)\nTutorials on tables:\n TD1 TD2 TD3 TD4   Possible solutions:\n TD1  Tutorials on machines (.ipynb):\n TP1 TP2 TP3 TP4  Prerequisites:\n basics of python numerical analysis with python  ","date":1549584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549584000,"objectID":"de5956401f940cb76f887a1041cf6cca","permalink":"https://grishchenko.org/project/numerical-optimization-2019/","publishdate":"2019-02-08T00:00:00Z","relpermalink":"/project/numerical-optimization-2019/","section":"project","summary":"Basic course on numerical optimization (theory and implementation)","tags":null,"title":"Optimisation Numérique","type":"project"},{"authors":["Dmitry Grishchenko","Franck Iutzeler","Jerome Malick","Massih-Reza Amini"],"categories":null,"content":"","date":1544400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544400000,"objectID":"f132701db51bc8a32298f87b75d6c975","permalink":"https://grishchenko.org/publication/spy/","publishdate":"2018-12-10T00:00:00Z","relpermalink":"/publication/spy/","section":"publication","summary":"In this paper, we present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Asynchronous Distributed Learning with Sparse Communications and Identification","type":"publication"},{"authors":["Filip Hanzley","Jakub Konecny","Nikolas Loizou","Peter Richtarik","Dmitry Grishchenko"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"fe803c6fb5367e9d85ea66ace3c379f6","permalink":"https://grishchenko.org/publication/gossip_nips/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/gossip_nips/","section":"publication","summary":"In this work we present a randomized gossip algorithm for solving the average consensus problem while at the same time protecting the information about the initial private values stored at the nodes. We give iteration complexity bounds for the method and perform extensive numerical experiments.","tags":["Source Themes"],"title":"A Privacy Preserving Randomized Gossip Algorithm via Controlled Noise Insertion","type":"publication"},{"authors":null,"categories":null,"content":"Universite Grenoble Alpes \u0026ndash; Master 2 MSIAM/MOSIG/etc. (TA 20h)\n Introduction The course website  ","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"a411a8e720b53547bab50bd6168e7c2b","permalink":"https://grishchenko.org/project/cdo2018/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/project/cdo2018/","section":"project","summary":"Incremental and Stochastic Optimization for Learning, Spark, Distributed Optimization.","tags":null,"title":"Convex and Distributed Optimization","type":"project"},{"authors":null,"categories":null,"content":"Universite Grenoble Alpes \u0026ndash; Master 2 MSIAM/MOSIG/etc. (TA 20h)\nReminder in Matrix analysis and optimization.\n Syllabus and Exercices: Pdf  Handwritten solutions:\n Matrix Part: Pdf  Notebooks: on GitHub of F. IUTZELER\n Chap 1: Python and NumPy Basics (look at that part before the practical sessions) Chap 2-1: Matrix Part Chap 2-2: Optimization Part  ","date":1537747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537747200,"objectID":"1c10cafb8699d148deb590f24ec03189","permalink":"https://grishchenko.org/project/refresher2018/","publishdate":"2018-09-24T00:00:00Z","relpermalink":"/project/refresher2018/","section":"project","summary":"This short course focuses on matrix analysis and optimization","tags":null,"title":"Refresher course$:$ Numerical Matrix Analysis and Optimization","type":"project"},{"authors":null,"categories":null,"content":"","date":1530779400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530779400,"objectID":"251156d15a85914e131ce841b673036f","permalink":"https://grishchenko.org/talk/ismp2018/","publishdate":"2018-07-05T08:30:00Z","relpermalink":"/talk/ismp2018/","section":"talk","summary":"We propose an efficient distributed algorithm for solving regularized learning problems. In a distributed framework with a master machine coordinating the computations of many slave machines, our proximal-gradient algorithm allows local computations and sparse communications from slaves to master. Furthermore, with the $\\ell_1$-regularizer, our approach automatically identifies the support of the solution, leading to sparse communications from master to slaves, with near-optimal support. We thus obtain an algorithm with two-way sparse communications.","tags":null,"title":"Distributed Optimization with Sparse Communications and Structure Identification","type":"talk"},{"authors":null,"categories":null,"content":"","date":1530207000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530207000,"objectID":"bf278e37718f1577c5c33e8a41ef1958","permalink":"https://grishchenko.org/talk/grenobleoptdays2018/","publishdate":"2018-06-28T17:30:00Z","relpermalink":"/talk/grenobleoptdays2018/","section":"talk","summary":"We propose an efficient distributed algorithm for solving regularized learning problems. In a distributed framework with a master machine coordinating the computations of many slave machines, our proximal-gradient algorithm allows local computations and sparse communications from slaves to master. Furthermore, with the $\\ell_1$-regularizer, our approach automatically identifies the support of the solution, leading to sparse communications from master to slaves, with near-optimal support. We thus obtain an algorithm with two-way sparse communications.","tags":null,"title":"Distributed Optimization with Sparse Communications and Structure Identification","type":"talk"},{"authors":null,"categories":null,"content":"","date":1522260000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522260000,"objectID":"515af549cc8722eeecd95f84ccb7d657","permalink":"https://grishchenko.org/talk/smai-mode/","publishdate":"2018-03-28T18:00:00Z","relpermalink":"/talk/smai-mode/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Distributed Optimization with Sparse Communications","type":"talk"},{"authors":["Filip Hanzley","Jakub Konecny","Nikolas Loizou","Peter Richtarik","Dmitry Grishchenko"],"categories":null,"content":"","date":1491523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491523200,"objectID":"93aa38dee6e39644e732d12d3e5d09a6","permalink":"https://grishchenko.org/publication/gossip_arxiv/","publishdate":"2017-04-07T00:00:00Z","relpermalink":"/publication/gossip_arxiv/","section":"publication","summary":"In this work we present a randomized gossip algorithm for solving the average consensus problem while at the same time protecting the information about the initial private values stored at the nodes. We give iteration complexity bounds for the method and perform extensive numerical experiments.","tags":null,"title":"Privacy preserving randomized gossip algorithms","type":"publication"},{"authors":null,"categories":null,"content":"","date":1480158000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480158000,"objectID":"e524bcbbce3c1f46c6b82e2ee8c248e2","permalink":"https://grishchenko.org/talk/mipt59/","publishdate":"2016-11-26T11:00:00Z","relpermalink":"/talk/mipt59/","section":"talk","summary":"In this work, we study the problem of restoring the correspondence matrix for measuring the flows on the links of a large computer network. This is an incorrect problem, which reduces to the choice of one of the solutions of an indefinite system of linear equations. In order for the task to become correct it is necessary to define the system in which in practice there are much more correspondences than links. In this work, we consider the MMI (minimal mutual information) model, in which the problem is reformulated in the language of convex composite optimization with the quadratic residual functional and the entropy penalty.","tags":null,"title":"Solving of Minimal Mutual Information Model Problem via Regularization of Dual Problem and Using Ellipsoid Method with Inexact Oracle","type":"talk"},{"authors":["Dmitry Grishchenko"],"categories":null,"content":"","date":1361145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361145600,"objectID":"342db4117416dfc36d0a30d28d0c1384","permalink":"https://grishchenko.org/publication/origami/","publishdate":"2013-02-18T00:00:00Z","relpermalink":"/publication/origami/","section":"publication","summary":"In this work we study the set of origami numbers, more precisely the set of coordinates of points that could be reached via paper folding. In the scope of this work we show that this set is a square and cubic roots extension of $\\mathbb{Q}$. It shows that using paper folds it is possible to get all the points that are reachable by ruler-and-compass construction and even more, for example angle trisection and doubling the cube.","tags":null,"title":"Origami: What One Can Get via Paper Folding (Rus)","type":"publication"}]