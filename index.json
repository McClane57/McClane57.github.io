[{"authors":["admin"],"categories":null,"content":"I am a third year PhD student at Université Grenoble Alpes supervised by Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini.\nI am interested in optimization algorithms for large-scale applications, more precisely in first-order ones that are focused on sparsification of communications in distributed setting.\nIn free time I am interested in basketball, workout, swimming, hiking, climbing. In addition, I am also a member of Russian first team in different logic puzzles and sudokucompetitions.\n","date":1600992000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1600992000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://grishchenko.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a third year PhD student at Université Grenoble Alpes supervised by Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini.\nI am interested in optimization algorithms for large-scale applications, more precisely in first-order ones that are focused on sparsification of communications in distributed setting.\nIn free time I am interested in basketball, workout, swimming, hiking, climbing. In addition, I am also a member of Russian first team in different logic puzzles and sudokucompetitions.","tags":null,"title":"Dmitry Grishchenko","type":"authors"},{"authors":["Gilles Bareilles","Yassine Laguel","Dmitry Grishchenko","Franck Iutzeler","Jerome Malick"],"categories":null,"content":"","date":1600992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600992000,"objectID":"fe83533dc886a00dddec0ef4cda64072","permalink":"https://grishchenko.org/publication/progressive_hedging/","publishdate":"2020-09-25T00:00:00Z","relpermalink":"/publication/progressive_hedging/","section":"publication","summary":"Progressive Hedging is a popular decomposition algorithm for solving multi-stage stochastic optimization problems. A computational bottleneck of this algorithm is that all scenario subproblems have to be solved at each iteration. In this paper, we introduce randomized versions of the Progressive Hedging algorithm able to produce new iterates as soon as a single scenario subproblem is solved. Building on the relation between Progressive Hedging and monotone operators, we leverage recent results on randomized fixed point methods to derive and analyze the proposed methods. Finally, we release the corresponding code as an easy-to-use Julia toolbox and report computational experiments showing the practical interest of randomized algorithms, notably in a parallel context. Throughout the paper, we pay a special attention to presentation, stressing main ideas, avoiding extra-technicalities, in order to make the randomized methods accessible to a broad audience in the Operations Research community.","tags":null,"title":" Randomized Progressive Hedging methods for Multi-stage Stochastic Programming","type":"publication"},{"authors":null,"categories":null,"content":"Title: Proximal Gradient Methods with Adaptive Subspace Samplings\nTogether with Gilles Bareilles, Yassine Laguel, Franck Iutzeler, and Jérôme Malick we propose an asynchronous version of the Progressive Hedging Algorithm(a popular strategy in multistage stochastic programming) that is able to compute an update as soon as a scenario subproblem is solved. Based on the similar arguments as in Asynchronous ADMM, we prove that the asynchronous version has the same convergence properties as the standard one and we release an easy-to-use Julia toolbox .\n","date":1600992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600992000,"objectID":"e9a443070f11032dee6ca1a7eab7c966","permalink":"https://grishchenko.org/post/new_progressive/","publishdate":"2020-09-25T00:00:00Z","relpermalink":"/post/new_progressive/","section":"post","summary":"Title: Proximal Gradient Methods with Adaptive Subspace Samplings\nTogether with Gilles Bareilles, Yassine Laguel, Franck Iutzeler, and Jérôme Malick we propose an asynchronous version of the Progressive Hedging Algorithm(a popular strategy in multistage stochastic programming) that is able to compute an update as soon as a scenario subproblem is solved. Based on the similar arguments as in Asynchronous ADMM, we prove that the asynchronous version has the same convergence properties as the standard one and we release an easy-to-use Julia toolbox .","tags":null,"title":"A new article about asynchronous progressive hedging","type":"post"},{"authors":null,"categories":null,"content":"Hello everyone,\nOn Tuesday, $29^{\\text{th}}$ September I will make a rehearsal for my Ph.D. defence.\nTitle: Proximal Optimization with Automatic Dimension Reduction for Large Scale Learning.\nAbstract: In this thesis, we develop a framework to reduce the dimensionality of composite optimization problems with sparsity inducing regularizers. Based on the identification property of proximal methods, we first develop a ``sketch-and-project” method that uses projections based on the structure of the correct point. This method allows to work with random low-dimensional subspaces instead of considering the full space in the cases when the final solution is sparse. Second, we place ourselves in the context of the delay-tolerant asynchronous proximal methods and use our dimension reduction technique to decrease the total size of communications. However, this technique is proven to converge only for well-conditioned problems both in theory in practice. Thus, we investigate wrapping it up into a proximal reconditioning framework. This leads to a theoretically backed algorithm that is guaranteed to cost less in terms of communications compared with a non-sparsified version; we show in practice that it implies faster runtime convergence when the sparsity of the problem is suficiently big.\nTalk will take a place in Batiment IMAG in the room $\\boldsymbol{106}$ (first floor) at $\\boldsymbol{14^{30}}$ on Tuesday, $\\boldsymbol{29^{\\text{th}}}$ September.\nAll are invited to attend.\nP.S. If you have no badge to enter the building please contact me in advance.\n","date":1600992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600992000,"objectID":"1738b7fbf04b13372c13885e76c8f4d9","permalink":"https://grishchenko.org/post/defence_rehearsal/","publishdate":"2020-09-25T00:00:00Z","relpermalink":"/post/defence_rehearsal/","section":"post","summary":"Hello everyone,\nOn Tuesday, $29^{\\text{th}}$ September I will make a rehearsal for my Ph.D. defence.\nTitle: Proximal Optimization with Automatic Dimension Reduction for Large Scale Learning.\nAbstract: In this thesis, we develop a framework to reduce the dimensionality of composite optimization problems with sparsity inducing regularizers. Based on the identification property of proximal methods, we first develop a ``sketch-and-project” method that uses projections based on the structure of the correct point.","tags":null,"title":"Rehearsal fo defence","type":"post"},{"authors":null,"categories":null,"content":"Grenoble INP \u0026ndash; masters of ENSIMAG (TA 25h)\nTutorials on tables:\n TD1 TD2 TD3 TD4  Possible solutions:\n TD1  Tutorials on machines (.ipynb):\n TP1 TP2 TP3 TP4  Prerequisites:\n basics of python numerical analysis with python  ","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581120000,"objectID":"3f4d806265480f674531e3f67e4f929e","permalink":"https://grishchenko.org/project/numerical-optimization-2020/","publishdate":"2020-02-08T00:00:00Z","relpermalink":"/project/numerical-optimization-2020/","section":"project","summary":"Basic course on numerical optimization (theory and implementation)","tags":null,"title":"Optimisation Numérique","type":"project"},{"authors":null,"categories":null,"content":"Title: Proximal Gradient Methods with Adaptive Subspace Samplings\nTogether with my supervisors Franck Iutzeler and Jérôme Malick we propose a randomized proximal gradient method that uses enforced by regularizer low dimensional structure of the solution to specify the subspaces and improve the rate in terms of dimensions explored.\n","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"ec788078aace513e76dd7d5b6da45c29","permalink":"https://grishchenko.org/post/new_adaptive_sampling/","publishdate":"2019-12-20T00:00:00Z","relpermalink":"/post/new_adaptive_sampling/","section":"post","summary":"Title: Proximal Gradient Methods with Adaptive Subspace Samplings\nTogether with my supervisors Franck Iutzeler and Jérôme Malick we propose a randomized proximal gradient method that uses enforced by regularizer low dimensional structure of the solution to specify the subspaces and improve the rate in terms of dimensions explored.","tags":null,"title":"A new article about random proximal subspace method with identification-based sampling","type":"post"},{"authors":["Dmitry Grishchenko","Franck Iutzeler","Jerome Malick"],"categories":null,"content":"","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"e12093bdecae21b5d36e34bbb06ed0ca","permalink":"https://grishchenko.org/publication/adaptive_sampling/","publishdate":"2019-12-20T00:00:00Z","relpermalink":"/publication/adaptive_sampling/","section":"publication","summary":"Many applications in machine learning or signal processing involve nonsmooth optimization problems. This nonsmoothness brings a low-dimensional structure to the optimal solutions. In this paper, we propose a randomized proximal gradient method harnessing this underlying structure. We introduce two key components$:$ i) a random subspace proximal gradient algorithm; ii) an identification-based sampling of the subspaces. Their interplay brings a significant performance improvement on typical learning problems in terms of dimensions explored.","tags":null,"title":"Proximal Gradient Methods with Adaptive Subspace Samplings","type":"publication"},{"authors":null,"categories":null,"content":"Title: Adaptive Catalyst for smooth convex optimization\nTogether with Anastasiya Ivanova, Egor Shulgin, and Alexander Gasnikov from MIPT research group, we propose the universal acceleration technique for adaptive methods for smooth convex but not strongly convex objective functions. It allows accelerating such well-known methods as Steepest Descent, Random Adaptive Coordinate Descent Method, and others where Lipschitz constant of the gradient is either unknown(expensive to compute) or changes a lot along the method trajectory.\n","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"3043f3c4f4e05ede25fb31ed65b76d98","permalink":"https://grishchenko.org/post/new_adaptive_catalyst/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/post/new_adaptive_catalyst/","section":"post","summary":"Title: Adaptive Catalyst for smooth convex optimization\nTogether with Anastasiya Ivanova, Egor Shulgin, and Alexander Gasnikov from MIPT research group, we propose the universal acceleration technique for adaptive methods for smooth convex but not strongly convex objective functions. It allows accelerating such well-known methods as Steepest Descent, Random Adaptive Coordinate Descent Method, and others where Lipschitz constant of the gradient is either unknown(expensive to compute) or changes a lot along the method trajectory.","tags":null,"title":"A new article about acceleration of adaptive methods.","type":"post"},{"authors":["Anastasiya Ivanova","Dmitry Pasechnyuk","Dmitry Grishchenko","Egor Shulgin","Alexander Gasnikov"],"categories":null,"content":"","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"11d4d85fdff7ee204b0464d1baf2ae19","permalink":"https://grishchenko.org/publication/adaptive_cat/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/publication/adaptive_cat/","section":"publication","summary":"In this paper, we present the generic framework that allows accelerating almost arbitrary non-accelerated deterministic and randomized algorithms for smooth convex optimization problems. The main approach of our \\emph{envelope} is the same as in \\textit{Catalyst} (Lin et., al., 2015)$:$ an accelerated proximal outer gradient method, which is used as an envelope for the non-accelerated inner method for the $\\ell_2$ regularized auxiliary problem. Our algorithm has two key differences$:$ $1)$ easily verifiable stopping criteria for inner algorithm; $2)$ the regularization parameter is capable of modification. As a result, the main contribution of this paper is a new framework that applies to adaptive inner algorithms$:$ Steepest Descent, Adaptive Coordinate Descent, Alternating Minimization. Moreover, in the non-adaptive case, our approach allows obtaining  Catalyst without a logarithmic factor, which appears in the standard Catalyst (Lin et., al., 2015, 2018).","tags":null,"title":"Adaptive Catalyst for smooth convex optimization","type":"publication"},{"authors":null,"categories":null,"content":"From 1 of November to 21 of December I am joining Peter Richtárik's team at KAUST as a visiting PhD student.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"7bd2aad041746a9c4cdacbebda5ede49","permalink":"https://grishchenko.org/post/kaust_visit/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/post/kaust_visit/","section":"post","summary":"From 1 of November to 21 of December I am joining Peter Richtárik's team at KAUST as a visiting PhD student.","tags":null,"title":"I am visiting Peter Richtárik at KAUST","type":"post"},{"authors":null,"categories":null,"content":"Universite Grenoble Alpes \u0026ndash; Master 2 MSIAM/MOSIG/etc. (TA 20h)\nReminder in Matrix analysis and optimization.\n Syllabus and Exercices: Pdf  Handwritten solutions:\n Matrix Part: Pdf  Notebooks: on GitHub of F. IUTZELER\n Chap 1: Python and NumPy Basics (look at that part before the practical sessions) Chap 2-1: Matrix Part Chap 2-2: Optimization Part  ","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"f5a35793f1ff92a0f803a922d9adcb57","permalink":"https://grishchenko.org/project/refresher2019/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/project/refresher2019/","section":"project","summary":"This short course focuses on matrix analysis and optimization","tags":null,"title":"Refresher course$:$ Numerical Matrix Analysis and Optimization","type":"project"},{"authors":null,"categories":null,"content":"Hello everyone,\nOn Monday, $23^{\\text{rd}}$ September Nikita Doikov will give a talk about some recent results on accelerated proximal methods.\nNikita is a PhD student at Université Catholique de Louvain, Belgium, where he is working on developing high-order optimization methods in the group of Prof. Yurii Nesterov.\nTitle: Proximal Method with Contractions for Smooth Convex Optimization.\nAbstract: In this work we study a construction of proximal accelerated methods for smooth convex optimization. At every step the method minimizes a sum of contracted objective and a regularizer in a form of Bregman divergence, by some auxiliary subroutine. We provide a simple practical stopping criterion for solving the subproblem and present complexity analysis for a general scheme. In the case when Tensor Method is used as internal method, we show accelerated rate of convergence in both convex and strongly convex composite cases with additional logarithmic factor as a cost for solving the subproblem.\nTalk will take a place in Batiment IMAG in the room $\\boldsymbol{106}$ (first floor) at $\\boldsymbol{14^{00}}$ on Monday, $\\boldsymbol{23^{\\text{rd}}}$ September.\nAll are invited to attend.\nP.S. If you have no badge to enter the building please contact me in advance.\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"4c4d45ef794d4c51c04fa2f06132a9a0","permalink":"https://grishchenko.org/post/nikita_visit/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/post/nikita_visit/","section":"post","summary":"Hello everyone,\nOn Monday, $23^{\\text{rd}}$ September Nikita Doikov will give a talk about some recent results on accelerated proximal methods.\nNikita is a PhD student at Université Catholique de Louvain, Belgium, where he is working on developing high-order optimization methods in the group of Prof. Yurii Nesterov.\nTitle: Proximal Method with Contractions for Smooth Convex Optimization.\nAbstract: In this work we study a construction of proximal accelerated methods for smooth convex optimization.","tags":null,"title":"Talk of Nikita Doikov","type":"post"},{"authors":null,"categories":null,"content":"","date":1565100300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565100300,"objectID":"f98655a5eef35cd0b9921d35c9cc0483","permalink":"https://grishchenko.org/talk/iccopt2019/","publishdate":"2019-08-06T14:05:00Z","relpermalink":"/talk/iccopt2019/","section":"talk","summary":"In this talk, we present a first-order optimization algorithm for distributed learning problems. We propose an efficient sparsification of proximal-gradient updates to lift the communication bottleneck of exchanging huge size updates between master machine and workers machines. Using a sketch-and-project technique with projections onto near-optimal low-dimensional subspaces, we get a significant performance improvement in terms of convergence with respect to the total size of communication.","tags":null,"title":"Identification-Based First-Order Algorithms for Distributed Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1562241900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562241900,"objectID":"c90e73712aa32fa58954a5841bec8220","permalink":"https://grishchenko.org/talk/spars2019/","publishdate":"2019-07-04T12:05:00Z","relpermalink":"/talk/spars2019/","section":"talk","summary":"Many machine learning and signal processing applications involve high-dimensional nonsmooth optimization problems. The nonsmoothness is essential as it brings a low-dimensional structure to the optimal solutions, as (block, rank, or variation) sparsity. In this work, we exploit this nonsmoothness to reduce the communication cost of optimization algorithms solving these problems in a distributed setting. We introduce two key ideas$:$ i) a random subspace descent algorithm along sparsity directions; ii) an adaptative subspace selection based on sparsity identification of the proximal operator. We get significant performance improvements in terms of convergence with respect to data exchanged.","tags":null,"title":"Distributed First-Order Optimization with Tamed Communications","type":"talk"},{"authors":null,"categories":null,"content":"","date":1554820200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554820200,"objectID":"afb4b9707224250e1756923c4da74815","permalink":"https://grishchenko.org/talk/dao2019/","publishdate":"2019-04-09T14:30:00Z","relpermalink":"/talk/dao2019/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Identify and Sparsify$:$ Distributed Optimization with Asynchronous Moderate Communications","type":"talk"},{"authors":null,"categories":null,"content":"    ","date":1553704200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553704200,"objectID":"6d0eb8ae48ccebeab0be4d447d47b040","permalink":"https://grishchenko.org/talk/osl2019/","publishdate":"2019-03-27T16:30:00Z","relpermalink":"/talk/osl2019/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Identify and Sparsify$:$ Distributed Optimization with Asynchronous Moderate Communications","type":"talk"},{"authors":null,"categories":null,"content":"Grenoble INP \u0026ndash; masters of ENSIMAG (TA 20h)\nTutorials on tables:\n TD1 TD2 TD3 TD4   Possible solutions:\n TD1  Tutorials on machines (.ipynb):\n TP1 TP2 TP3 TP4  Prerequisites:\n basics of python numerical analysis with python  ","date":1549584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549584000,"objectID":"de5956401f940cb76f887a1041cf6cca","permalink":"https://grishchenko.org/project/numerical-optimization-2019/","publishdate":"2019-02-08T00:00:00Z","relpermalink":"/project/numerical-optimization-2019/","section":"project","summary":"Basic course on numerical optimization (theory and implementation)","tags":null,"title":"Optimisation Numérique","type":"project"},{"authors":["Dmitry Grishchenko","Franck Iutzeler","Jerome Malick","Massih-Reza Amini"],"categories":null,"content":"","date":1544400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544400000,"objectID":"f132701db51bc8a32298f87b75d6c975","permalink":"https://grishchenko.org/publication/spy/","publishdate":"2018-12-10T00:00:00Z","relpermalink":"/publication/spy/","section":"publication","summary":"In this paper, we present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Asynchronous Distributed Learning with Sparse Communications and Identification","type":"publication"},{"authors":["Filip Hanzley","Jakub Konecny","Nikolas Loizou","Peter Richtarik","Dmitry Grishchenko"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"fe803c6fb5367e9d85ea66ace3c379f6","permalink":"https://grishchenko.org/publication/gossip_nips/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/gossip_nips/","section":"publication","summary":"In this work we present a randomized gossip algorithm for solving the average consensus problem while at the same time protecting the information about the initial private values stored at the nodes. We give iteration complexity bounds for the method and perform extensive numerical experiments.","tags":["Source Themes"],"title":"A Privacy Preserving Randomized Gossip Algorithm via Controlled Noise Insertion","type":"publication"},{"authors":null,"categories":null,"content":"Universite Grenoble Alpes \u0026ndash; Master 2 MSIAM/MOSIG/etc. (TA 20h)\n Introduction The course website  ","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"a411a8e720b53547bab50bd6168e7c2b","permalink":"https://grishchenko.org/project/cdo2018/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/project/cdo2018/","section":"project","summary":"Incremental and Stochastic Optimization for Learning, Spark, Distributed Optimization.","tags":null,"title":"Convex and Distributed Optimization","type":"project"},{"authors":null,"categories":null,"content":"Universite Grenoble Alpes \u0026ndash; Master 2 MSIAM/MOSIG/etc. (TA 20h)\nReminder in Matrix analysis and optimization.\n Syllabus and Exercices: Pdf  Handwritten solutions:\n Matrix Part: Pdf  Notebooks: on GitHub of F. IUTZELER\n Chap 1: Python and NumPy Basics (look at that part before the practical sessions) Chap 2-1: Matrix Part Chap 2-2: Optimization Part  ","date":1537747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537747200,"objectID":"1c10cafb8699d148deb590f24ec03189","permalink":"https://grishchenko.org/project/refresher2018/","publishdate":"2018-09-24T00:00:00Z","relpermalink":"/project/refresher2018/","section":"project","summary":"This short course focuses on matrix analysis and optimization","tags":null,"title":"Refresher course$:$ Numerical Matrix Analysis and Optimization","type":"project"},{"authors":null,"categories":null,"content":"","date":1530779400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530779400,"objectID":"251156d15a85914e131ce841b673036f","permalink":"https://grishchenko.org/talk/ismp2018/","publishdate":"2018-07-05T08:30:00Z","relpermalink":"/talk/ismp2018/","section":"talk","summary":"We propose an efficient distributed algorithm for solving regularized learning problems. In a distributed framework with a master machine coordinating the computations of many slave machines, our proximal-gradient algorithm allows local computations and sparse communications from slaves to master. Furthermore, with the $\\ell_1$-regularizer, our approach automatically identifies the support of the solution, leading to sparse communications from master to slaves, with near-optimal support. We thus obtain an algorithm with two-way sparse communications.","tags":null,"title":"Distributed Optimization with Sparse Communications and Structure Identification","type":"talk"},{"authors":null,"categories":null,"content":"","date":1530207000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530207000,"objectID":"bf278e37718f1577c5c33e8a41ef1958","permalink":"https://grishchenko.org/talk/grenobleoptdays2018/","publishdate":"2018-06-28T17:30:00Z","relpermalink":"/talk/grenobleoptdays2018/","section":"talk","summary":"We propose an efficient distributed algorithm for solving regularized learning problems. In a distributed framework with a master machine coordinating the computations of many slave machines, our proximal-gradient algorithm allows local computations and sparse communications from slaves to master. Furthermore, with the $\\ell_1$-regularizer, our approach automatically identifies the support of the solution, leading to sparse communications from master to slaves, with near-optimal support. We thus obtain an algorithm with two-way sparse communications.","tags":null,"title":"Distributed Optimization with Sparse Communications and Structure Identification","type":"talk"},{"authors":null,"categories":null,"content":"","date":1522260000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522260000,"objectID":"515af549cc8722eeecd95f84ccb7d657","permalink":"https://grishchenko.org/talk/smai-mode/","publishdate":"2018-03-28T18:00:00Z","relpermalink":"/talk/smai-mode/","section":"talk","summary":"We present an asynchronous optimization algorithm for distributed learning, that efficiently reduces the communications between a master and working machines by randomly sparsifying the local updates. This sparsification allows to lift the communication bottleneck often present in distributed learning setups where computations are performed by workers on local data while a master machine coordinates their updates to optimize a global loss. We prove that despite its sparse asynchronous communications, our algorithm allows for a fixed stepsize and benefits from a linear convergence rate in the strongly convex case. Moreover, for $\\ell_1$-regularized problems, this algorithm identifies near-optimal sparsity patterns, so that all communications eventually become sparse. We furthermore leverage on this identification to improve our sparsification technique. We illustrate on real and synthetic data that this algorithm converges faster in terms of data exchanges.","tags":null,"title":"Distributed Optimization with Sparse Communications","type":"talk"},{"authors":["Filip Hanzley","Jakub Konecny","Nikolas Loizou","Peter Richtarik","Dmitry Grishchenko"],"categories":null,"content":"","date":1491523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491523200,"objectID":"93aa38dee6e39644e732d12d3e5d09a6","permalink":"https://grishchenko.org/publication/gossip_arxiv/","publishdate":"2017-04-07T00:00:00Z","relpermalink":"/publication/gossip_arxiv/","section":"publication","summary":"In this work we present a randomized gossip algorithm for solving the average consensus problem while at the same time protecting the information about the initial private values stored at the nodes. We give iteration complexity bounds for the method and perform extensive numerical experiments.","tags":null,"title":"Privacy Preserving Randomized Gossip Algorithms","type":"publication"},{"authors":null,"categories":null,"content":"","date":1480158000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480158000,"objectID":"e524bcbbce3c1f46c6b82e2ee8c248e2","permalink":"https://grishchenko.org/talk/mipt59/","publishdate":"2016-11-26T11:00:00Z","relpermalink":"/talk/mipt59/","section":"talk","summary":"In this work, we study the problem of restoring the correspondence matrix for measuring the flows on the links of a large computer network. This is an incorrect problem, which reduces to the choice of one of the solutions of an indefinite system of linear equations. In order for the task to become correct it is necessary to define the system in which in practice there are much more correspondences than links. In this work, we consider the MMI (minimal mutual information) model, in which the problem is reformulated in the language of convex composite optimization with the quadratic residual functional and the entropy penalty.","tags":null,"title":"Solving of Minimal Mutual Information Model Problem via Regularization of Dual Problem and Using Ellipsoid Method with Inexact Oracle","type":"talk"},{"authors":["Dmitry Grishchenko"],"categories":null,"content":"","date":1361145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361145600,"objectID":"342db4117416dfc36d0a30d28d0c1384","permalink":"https://grishchenko.org/publication/origami/","publishdate":"2013-02-18T00:00:00Z","relpermalink":"/publication/origami/","section":"publication","summary":"In this work we study the set of origami numbers, more precisely the set of coordinates of points that could be reached via paper folding. In the scope of this work we show that this set is a square and cubic roots extension of $\\mathbb{Q}$. It shows that using paper folds it is possible to get all the points that are reachable by ruler-and-compass construction and even more, for example angle trisection and doubling the cube.","tags":null,"title":"Origami: What One Can Get via Paper Folding (Rus)","type":"publication"}]